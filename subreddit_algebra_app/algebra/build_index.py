"""
Index subreddit vectors in a Ball Tree to support efficient K-Nearest-Neighbors search.
When called as __main__, this saves the built index into the 'search_index.pkl'.
"""
import os

import marisa_trie
import numpy as np
import pandas as pd
from sklearn.neighbors import BallTree
from sklearn.preprocessing import normalize


def load_csv(filepath):
    """Load csv generated by processData.sql, transforming it from reddit/reddit/#overlap pairs to features

    TODO: Add instructions on generating csv from processData.sql
    """
    # t1_subreddit, t2_subreddit, NumOverlaps
    df = pd.read_csv(filepath, memory_map=True)
    # normalize all column names
    df['t1_subreddit'] = df['t1_subreddit'].str.lower()
    df['t2_subreddit'] = df['t2_subreddit'].str.lower()

    # Convert so that there is a row for each t1_subreddit, and each column is NumOverlaps between it and a t2_subreddit
    t1_overlaps = pd.crosstab(
        index=df['t1_subreddit'],
        columns=df['t2_subreddit'],
        values=df['NumOverlaps'],
        aggfunc=sum,
    )

    # Any t1_subreddit/t2_subreddit pairs with no overlaps had NaN. Replace with 0
    t1_overlaps.fillna(0, inplace=True)

    # Get a transposed view, a row for each t2_subreddit where each column is a NumOverlaps between it and t1_subreddit
    t2_overlaps = t1_overlaps.transpose()

    return t2_overlaps


def pmi(df):
    """Calculate the positive pointwise mutal information score for each entry

    https://en.wikipedia.org/wiki/Pointwise_mutual_information

    We use the log2( p(y|x)/p(y) ), y being the column, x being the row
    """
    # Get numpy array from pandas df
    arr = df.as_matrix()

    # p(y|x) probability of each t1 overlap within the row subreddit
    row_totals = arr.sum(axis=1)
    prob_cols_given_row = (arr.T / row_totals).T

    # p(y) probability of each t1 subreddit in the total set
    col_totals = arr.sum(axis=0)
    prob_of_cols = col_totals / sum(col_totals)

    # PMI: log( p(y|x) / p(y) )
    # This is the same data, normalized
    _pmi = np.log2(prob_cols_given_row / prob_of_cols)
    _pmi[_pmi < 0] = 0

    return _pmi


def index_data(df, ndarr):
    """Build an index to optimize K-Nearest-Neighbors search on the df

    Accepts a DataFrame of the data (for the column and row names), and a numpy arr of the same data. ndarr is
    normalized in place to save memory.

    Based on this StackOverflow answer, Euclidean distance of normalized vectors is a metric equivalent to cosine
    similarity https://goo.gl/N6cpza. This exploits that to use the builting sklearn l2 norm Distance Metric, after
    first making sure to normalize the data.

    This index will only return accurate results if the query is a vector with unit norm.
    """
    # In-place l2 normalization
    normalize(ndarr, copy=False)

    # Create the index using a Ball Tree, default leaf nodes, default 'l2' metric
    tree = BallTree(ndarr)

    return tree


def index_subreddit_names(df):
    """Return a trie of all subreddit names we have data for"""
    names = marisa_trie.Trie(df.index.values.tolist())
    return names


def data_filepath(p):
    data_directory = os.environ.get('SUBREDDIT_ALGEBRA_DATA_DIR', 'output')
    return '{}/{}'.format(data_directory, p)


if __name__ == '__main__':
    import pickle
    import sys

    # Relative import
    import constants

    # Filepath to the CSV of processData.sql output
    filepath = sys.argv[1]

    # Load and normalize the data
    df = load_csv(filepath)
    # Get an ndarray of pointwise mutual information
    ndarr = pmi(df)
    # Index the data
    tree = index_data(df, ndarr)
    # Build a trie of the subreddit names for completions
    names_trie = index_subreddit_names(df)

    # Maintain an ordered list of names and a dictionary of names to indexes so we can translate between user
    # (names) and the model (indexes) both ways.
    ordered_names = df.index.values.tolist()
    names_to_indexes = {
        name: ix
        for ix, name in enumerate(ordered_names)
    }

    # filepath, object tuples
    # Pickle all in the .gitignored output/ folder
    # TODO: Accept the folder as an argument
    to_pickle = (
        (constants.PMI_FILE, ndarr),
        (constants.INDEX_FILE, tree),
        (constants.NAMES_TRIE_FILE, names_trie),
        (constants.ORDERED_NAMES_FILE, ordered_names),
        (constants.NAMES_TO_INDEXES_FILE, names_to_indexes),
    )

    for filepath, obj in to_pickle:
        with open(data_filepath(filepath), 'wb') as f:
            pickle.dump(obj, f)
